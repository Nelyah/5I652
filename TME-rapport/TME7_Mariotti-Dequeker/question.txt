1.4 Questions: 

2. La convolution permet un gain important de performance. Dès lors qu'on
travaille sur des données de taille significative, des couches fully-connected
ont une complexité qui croit de façon exponentielle. La convolution permettra
d'appliquer un nombre limité de filtres et d'avoir un temps raisonnable de
calcul. Pour la convolution, le nombre de filtres est limitant. Si trop de
filtres sont définis, alors la problématique que l'on aura se rapprochera de
celle qu'on avait avec le réseau fully connected. Ainsi on privilégie un petit
nombre de filtres donc un grand nombre de couches.

-- 
Filtres de conv prennent en compte l'info spatiale. Organisation géométrique despixels entre eux.
Meilleure reconnaissance de feature dans l'image, pas plus de paramètres.
Limite principale: Etre local ne permet pas de mettre en relation des features éloignées dans l'image

3. Le pooling permet de réduire la taille des données. Cela permet entre autres
d'avoir une approximation locale qui peut donner une certaine résistatnce à la
translation. Réduire la taille permet d'augmenter la profondeur, et donc le
nombre de features.

--
Permet de réduire les dimensions spatiales.


4. Dans ce cas là, les étapes de convolution vont donner un output qui sera une
matrice plus grande que celle prévue au départ. Le problème qui pourrait
arriver serait à une étape fully-connected où les tailles de matrice ne
correspondraient pas.
--


Partie 2:


2.10. 
conv1 : 32*32*32 -- on aura 3*5*5*32 poids à apprendre
pool1:  16*16*32 -- Pas d'apprentissage
conv2:  16*16*64 -- 32*5*5*64 poids à apprendre
pool2:  8*8*64   -- Pas d'apprentissage
conv3:  8*8*64   -- 64*5*5*64 poids à apprendre
pool3:  4*4*64   -- Pas d'apprentissage
fc4:    1000     -- 1000*4*4*64
fc5:    10       -- 10000

-- (+1 coefficient à apprendre à chaque convo pour le biais) * K (pour K filtres)

Le nombre de poids à apprendre augmente avec la profondeur 
dans notre cas.

2.14. Dans le cas des données de train, l'optimiser est utilisé pour faire une descente de gradiant. Dans le cas des données de test, aucun optimiser n'est passé, donc aucune modification des poids.

2.16 Dans le cas où on a un learning rate trop faible, l'apprentissage
est trop lent et ne peut pas atteindre une valeur de loss suffisamment 
basse. Dans le cas où il est trop grand en revanche l'apprentissage est trop
instable (le Loss peut même augmenter dans certains cas). 
Un batch-size élevé permet d'augmenter la vitesse d'exécution, mais au risque de trop 'aplatir' le gradient. Un batch-size petit réduit la vitesse de calcul. Un batch de trop petite taille risque de donner une descente trop stochastiques. Il faut donc trouver une valeur entre les deux.

2.18. Si on augmente le nombre d'epochs au delà de 5, on observe que le modèle fait du sur-apprentissage: La Loss du Test augmente alors que la Loss du Train diminue constamment. Il faudrait donc augmenter ses performances de généralisation. 

Partie 3:
=========

-- Normalisation
Normalisation: Rammener les images vers une valeur moyenne. Cela permet d'homogénéiser les données, et donc d'avoir des données plus générales. Les outlyers seront ainsi traités normalement.

--Crop
Crop, ça fonctionne. Le but est d'appliquer des transformations pour pallier un faible nombre d'images dans le dataset, et une meilleure résistance aux modifications. De la même façon, le flip va permettre de reconnaitre des images qui seraient renversées. 


--Smooth
On observe sur les images des résutlats qui sont beaucoup plus réguliers. Cela fonctionne car le gradient aura un comportement moins stochastique et va permettre d'affiner le learning rate au fur et à mesure de l'apprentissage. 


--dropout
Le dropout permet d'éviter le surapprentissage. Des connexions vont être désactivées à chaque backpropagation. On peut le voir sur les graphiques que le jeu de test plafonne à une accuracy et loss. Un neurone va ainsi moins se spécialiser.


--batch normalisation
Applique une transformation supplémentaire entre les couches permettant de généraliser l'apprentissage.
